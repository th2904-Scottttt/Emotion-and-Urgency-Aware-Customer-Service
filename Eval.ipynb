{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3220d6c9-c721-4f63-81c5-e46c0f8a0a1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /venv/main/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /venv/main/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /venv/main/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b2ba7-5f5d-4837-bc51-186541314b2f",
   "metadata": {},
   "source": [
    "## Load fine tuned pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdff8acc-9cfc-4f75-97ca-00901b37e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_dir = \"./qwen2.5-cai-dpo-final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de13832-3d03-4000-a84f-8c23b3c61c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "#Base model name\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "#Load base model ONCE\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "FINETUNED_PATH = \"./qwen2.5-cai-dpo-final\"\n",
    "\n",
    "#Load PEFT model\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINETUNED_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8543a0c-7f67-470c-998a-4e482222ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_input', 'emotion', 'urgency', 'draft_reply', 'revised_reply', 'critiques', 'messages']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=\"cai_pairs.jsonl\")[\"train\"]\n",
    "\n",
    "def add_messages(example):\n",
    "    #retrieve emtion and urgency from example\n",
    "    emotion = example['emotion']\n",
    "    urgency = example['urgency']\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are a GENERAL helpful assistant, NOT a ticket agent\"\n",
    "        \" NEVER mention company names, websites, or booking services\"\n",
    "        f\"The user's detected emotion is '{emotion}'.\"\n",
    "        f\"The urgency level is {urgency} (1=low, 2=medium, 3=high).\"\n",
    "        \"ANSWER THE QUESTION using ONLY the Context and prompt provided.\"\n",
    "        \"NEVER make up information. NEVER mention companies or websites.\"\n",
    "        \"Respond naturally and helpfully, but do NOT apply any extra constitutional self-critique or revision here. This is only the initial draft reply.\"\n",
    "        \"Maximum 2-3 sentences.\"\n",
    "    )\n",
    "\n",
    "    example[\"messages\"] = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": example[\"user_input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"revised_reply\"]},\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(add_messages)\n",
    "\n",
    "orig_cols = ds.column_names\n",
    "print(orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac44e9f5-cffc-4b03-837f-4654ee1523b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "client = OpenAI()   # Client automatically reads the environment variable\n",
    "print(\"Client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25115582-438e-4da5-8c7f-857ab809a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_dpo_example_with_judge(cai_pair: dict,\n",
    "                                judge_model: str = \"gpt-4o-mini\") -> dict:\n",
    "    \"\"\"\n",
    "    Use a powerful reference model (gpt-4o-mini) to decide which reply\n",
    "    is better between draft_reply and revised_reply, based on:\n",
    "      - system prompt (emotion + urgency)\n",
    "      - user input\n",
    "    and return a single DPO example.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Extract fields from CAI pair\n",
    "    user_input   = cai_pair[\"user_input\"]\n",
    "    emotion      = cai_pair[\"emotion\"]\n",
    "    urgency      = cai_pair[\"urgency\"]\n",
    "    draft_reply  = cai_pair[\"draft_reply\"]\n",
    "    revised_reply = cai_pair[\"revised_reply\"]\n",
    "\n",
    "    # 2. Build system prompt (same style as your SFT prompt)\n",
    "    system_msg = (\n",
    "        \"You are a GENERAL helpful assistant, NOT a ticket agent.\\n\"\n",
    "        \"NEVER mention company names, websites, or booking services.\\n\"\n",
    "        f\"The user's detected emotion is '{emotion}'.\\n\"\n",
    "        f\"The urgency level is '{urgency}'.\\n\"\n",
    "        \"ANSWER THE QUESTION using ONLY the context and prompt provided.\\n\"\n",
    "        \"NEVER make up information. NEVER mention companies or websites.\\n\"\n",
    "        \"Respond naturally and helpfully.\\n\"\n",
    "        \"Maximum 2–3 sentences.\"\n",
    "    )\n",
    "\n",
    "    # 3. Build judge prompt\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert evaluator for conversational AI systems.\n",
    "Your job is to choose which assistant reply is better.\n",
    "\n",
    "Evaluation criteria:\n",
    "- Alignment with the system prompt\n",
    "- Tone matches emotion: '{emotion}'\n",
    "- Urgency handling matches level: '{urgency}'\n",
    "- No hallucinated assumptions\n",
    "- Concise, friendly, and helpful\n",
    "\n",
    "SYSTEM PROMPT GIVEN TO THE MODEL:\n",
    "\\\"\\\"\\\"{system_msg}\\\"\\\"\\\"\n",
    "\n",
    "USER INPUT:\n",
    "\\\"\\\"\\\"{user_input}\\\"\\\"\\\"\n",
    "\n",
    "RESPONSE A (Draft):\n",
    "\\\"\\\"\\\"{draft_reply}\\\"\\\"\\\"\n",
    "\n",
    "RESPONSE B (Revised):\n",
    "\\\"\\\"\\\"{revised_reply}\\\"\\\"\\\"\n",
    "\n",
    "Which reply is better? Answer with ONLY a single character: 'A' or 'B'.\n",
    "\"\"\"\n",
    "\n",
    "    # 4. Ask judge model\n",
    "    resp = client.chat.completions.create(\n",
    "        model=judge_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "        max_tokens=1,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    decision = resp.choices[0].message.content.strip().upper()\n",
    "\n",
    "    # Fallback safety\n",
    "    if decision not in (\"A\", \"B\"):\n",
    "        decision = \"B\"\n",
    "\n",
    "    if decision == \"A\":\n",
    "        chosen, rejected = draft_reply, revised_reply\n",
    "    else:\n",
    "        chosen, rejected = revised_reply, draft_reply\n",
    "\n",
    "    # 5. Build DPO training prompt (same format as before)\n",
    "    prompt = (\n",
    "        f\"System: {system_msg}\\n\"\n",
    "        f\"User: {user_input}\\n\"\n",
    "        \"Assistant:\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen_response\": chosen,\n",
    "        \"rejected_response\": rejected,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b619e6d-24d9-4813-bef5-51b17568f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cai_pairs_and_build_dpo_dataset_with_judge(\n",
    "    cai_path: str,\n",
    "    judge_model: str = \"gpt-4o-mini\",\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Read cai_pairs JSONL and use gpt-4o-mini to label each pair\n",
    "    with chosen / rejected responses for DPO.\n",
    "    \"\"\"\n",
    "    cai_pairs = []\n",
    "    with open(cai_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            cai_pairs.append(json.loads(line))\n",
    "\n",
    "    dpo_examples = []\n",
    "    for p in tqdm(cai_pairs, desc=\"Labeling with judge model\"):\n",
    "        dpo_examples.append(make_dpo_example_with_judge(p, judge_model))\n",
    "\n",
    "    dpo_ds = Dataset.from_list(dpo_examples)\n",
    "    return dpo_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1db457-ec9e-4079-aba7-a7efe7f4f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling with judge model: 100%|██████████| 1999/1999 [15:58<00:00,  2.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"System: You are a GENERAL helpful assistant, NOT a ticket agent.\\nNEVER mention company names, websites, or booking services.\\nThe user's detected emotion is 'happy'.\\nThe urgency level is 'low'.\\nANSWER THE QUESTION using ONLY the context and prompt provided.\\nNEVER make up information. NEVER mention companies or websites.\\nRespond naturally and helpfully.\\nMaximum 2–3 sentences.\\nUser: hi good morning\\nAssistant:\", 'chosen_response': 'Hey there! Good morning! :)', 'rejected_response': 'Hello! Welcome to AT&T Customer Service. How may I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dpo_ds = load_cai_pairs_and_build_dpo_dataset_with_judge(\"cai_pairs.jsonl\")\n",
    "print(dpo_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be65a68a-f631-4530-baf9-9a873c590f46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Dataset' object has no attribute 'head'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "237b8c56-50cd-418a-ac5e-d371482fdf16",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a88a0b-22dd-4bb1-b61b-a68647902117",
   "metadata": {},
   "source": [
    "#### Valid Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29d92ecf-03f2-41bb-b15b-1c8a2d2ed9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ace849f-6eb5-42da-a1ec-672e4ae1f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid comparisons: 411/1000, Revised preferred rate = 41.10%\n"
     ]
    }
   ],
   "source": [
    "total = len(ds)          # 2000\n",
    "sample_size = 1000\n",
    "indices = np.random.choice(total, size=sample_size, replace=False)\n",
    "\n",
    "def normalize(txt: str) -> str:\n",
    "    return \" \".join(txt.split())\n",
    "\n",
    "n_revised_preferred = 0\n",
    "\n",
    "for idx in indices:\n",
    "    cai_ex = ds[idx]\n",
    "    dpo_ex = dpo_ds[idx]\n",
    "\n",
    "    revised = normalize(cai_ex[\"revised_reply\"])\n",
    "    chosen  = normalize(dpo_ex[\"chosen_response\"])\n",
    "\n",
    "    if chosen == revised:\n",
    "        n_revised_preferred += 1\n",
    "\n",
    "rate = n_revised_preferred / sample_size\n",
    "print(f\"Valid comparisons: {n_revised_preferred}/{sample_size}, Revised preferred rate = {rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d93fb7-7623-459d-bd32-b3cb3d436d23",
   "metadata": {},
   "source": [
    "#### Percentage of critiques that identified tone/urgency issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffc468db-4597-4cce-adb8-f8c37d22e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'principle_name': 'Global_Safety',\n",
       "  'condition': 'any',\n",
       "  'critic_instruction': \"Critique the assistant's reply only for safety issues. Does it include harmful, illegal, self-harm, dangerous, or expert-only advice? Point out any unsafe content and explain what is wrong with it.\",\n",
       "  'critique': \"Thank you for reaching out to us. We're glad to be able to help you in this regards. Could you please let me know your city name and zip code?\"},\n",
       " {'principle_name': 'Global_Helpfulness_Clarity',\n",
       "  'condition': 'any',\n",
       "  'critic_instruction': \"Critique the assistant's reply for helpfulness and clarity. Does it answer the user's question directly? Is it specific, concise, and easy to understand? Identify vagueness, over-verbosity, or irrelevant content.\",\n",
       "  'critique': 'Surely, could you please help me with your query?'},\n",
       " {'principle_name': 'Emotion_Positive_Tone',\n",
       "  'condition': 'emotion_positive',\n",
       "  'critic_instruction': \"The user's detected emotion is happy. Critique whether the reply appropriately supports this positive emotion. Does it sound too flat, negative, or unaligned with the user's good mood?\",\n",
       "  'critique': \"Surely! I'll be glad to help you in this regards. Could you please let me know your name and last 4 digits of account number?\"},\n",
       " {'principle_name': 'Urgency_Low',\n",
       "  'condition': 'urgency_low',\n",
       "  'critic_instruction': \"The user's urgency level is 'low'. Critique whether the reply maintains a relaxed tone and avoids unnecessary escalation or pressure. Check if it still provides useful information.\",\n",
       "  'critique': 'Sure, thank you for reaching out to us. Could you please help me with your name and last 4 digits of SSN?'},\n",
       " {'principle_name': 'Honesty_and_Uncertainty',\n",
       "  'condition': 'any',\n",
       "  'critic_instruction': \"Critique whether the reply is honest about the model's limitations. Does it hallucinate facts, make unsupported claims, or hide uncertainty where it should be stated?\",\n",
       "  'critique': \"Thank you for reaching out to us. We're glad to serve you.\"}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['critiques']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ba78e47-39ff-416a-8e06-8d92643041af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def has_tone_or_urgency_change(example, threshold=0.4):\n",
    "    \"\"\"\n",
    "    Return True if the emotion/urgency critique is substantially\n",
    "    different from the draft reply (i.e., likely identified an issue).\n",
    "    \"\"\"\n",
    "    draft = example[\"draft_reply\"]\n",
    "\n",
    "    # collect all emotion/urgency critiques\n",
    "    emo_urg_crits = [\n",
    "        c[\"critique\"]\n",
    "        for c in example[\"critiques\"]\n",
    "        if \"emotion\" in c.get(\"principle_name\", \"\").lower()\n",
    "        or \"urgency\" in c.get(\"principle_name\", \"\").lower()\n",
    "    ]\n",
    "\n",
    "    if not emo_urg_crits:\n",
    "        return False  # no emo/urg critique at all\n",
    "\n",
    "    # concatenate them (often there is just one of each)\n",
    "    crit_text = \" \".join(emo_urg_crits)\n",
    "\n",
    "    # similarity between draft and critique text\n",
    "    sim = SequenceMatcher(None, draft, crit_text).ratio()\n",
    "\n",
    "    # if similarity is low, treat as \"issue identified\"\n",
    "    return sim < threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a15e0485-166b-42fa-9e35-1ada8b011263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critique identified tone/urgency issues in 94.25% of cases.\n"
     ]
    }
   ],
   "source": [
    "flags = np.array([has_tone_or_urgency_change(ex) for ex in ds])\n",
    "\n",
    "issue_rate = flags.mean()\n",
    "print(f\"Critique identified tone/urgency issues in {issue_rate:.2%} of cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49258f51-e0ce-42f5-bd55-0a235ee95ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
